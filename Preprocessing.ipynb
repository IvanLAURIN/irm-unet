{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bedb31d-1df1-4f73-bb6b-405fce2661ac",
   "metadata": {},
   "source": [
    "# Pré-traîtement des images de la base BraTS 2019 pour l'entraînement de réseaux de neurones dédiés à la segmentation de tumeurs\n",
    "\n",
    "Le code présenté ci-dessous permet le redimensionnement et la normalisation des images issues de la base de données BraTS 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb2eeb-1f1e-4ba2-ad26-93b815f8f6de",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6853329-3fd7-41f7-a24e-552a7b4504cc",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Ce code a été conçu et testé avec la configuration suivante : \n",
    "-  Python 3.10\n",
    "-  Pandas 1.5\n",
    "-  NiBabel 5.0\n",
    "-  NiLearn 0.10\n",
    "\n",
    "Le téléchargement et la décompression des données sont assurés par les fonctions **wget** et **unzip** qui peuvent être installées dans Windows à l'aide du logiciel ***Cygwin***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66826b74-837f-449d-9728-4ed3e2aab6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn.image import resample_img\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c4dfe-e19f-4703-b920-9b828d3b5590",
   "metadata": {},
   "source": [
    "### Téléchargement des données\n",
    "\n",
    "Les données de la base sont récupérées sur les serveurs AWS de l'école Blent AI, puis décompressées dans le dossier d'où est lancé ce programme. L'archive compressée est ensuite supprimée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37848b-7af4-4cdf-a542-943457a3502f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Téléchargement des données\n",
    "!wget -q https://blent-learning-user-ressources.s3.eu-west-3.amazonaws.com/projects/60fb61/brats_2019.zip\n",
    "!unzip -qq brats_2019.zip -d ./\n",
    "!rm brats_2019.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36503cc3-ef61-4414-acda-789990ad2cd5",
   "metadata": {},
   "source": [
    "### Description des données originales\n",
    "\n",
    "Les données de la base BraTS 2019 sont organisées de la façon suivante :  Pour chaque cas (335 cas au total), la base contient 5 images au format .nii, qui sont : \n",
    "-  L'image IRM avec pondération T1 (**'t1'**)\n",
    "-  L'image IRM avec pondération T1 et augmentation du contraste (**'t1ce'**)\n",
    "-  L'image IRM avec pondération T2 (**'t2'**)\n",
    "-  L'image IRM avec pondération T2 et atténuation des fluides par inversion-récupération (**'flair'**)\n",
    "-  La carte de segmentation (**'seg'**)\n",
    "\n",
    "Ces données sont présentées sous la forme de volumes 3D de dimensions (240, 240, 155) voxels.\n",
    "\n",
    "Les images IRM comportent toutes les acquisitions du cerveau, qui a été isolé (le crâne et les tissus autres que ceux du cerveau ont été retirés). Les intensités sont représentées au format *int16*, et valent 0 patrout en dehors du cerveau. Ces données ne sont pas normalisées, ainsi d'un cas à l'autre la plage de dynamique de l'intensité peut être très variable.\n",
    "\n",
    "La carte de segmentation est aussi au format *int16*, et vaut 0 partout en dehors des tissus tumoraux. Ces tissus ont été segmentés à la main, en 3 catégories. Le label associé à une catégorie correspond à la valeur de la carte de segmentation en un point appartenant à un tissu de cette catégorie. Ces catégories sont : \n",
    "-  *Necrotic and non-enhancing tumor core* - (Label 1)\n",
    "-  *Peritumoral edema* - (Label 2)\n",
    "-  *GD-enhancing tumor* - (Label 4)\n",
    "\n",
    "Se reporter à la [page de description du dataset BraTS 2019](https://www.med.upenn.edu/cbica/brats2019/data.html) pour plus d'informations.\n",
    "\n",
    "Pour chaque cas, toutes ces images ont été recalées dans un repère commun, mais l'isolation des tissus du cerveau ayant été faite de façon indépendante pour chaque image, leurs contours sont parfois différents. De plus, la plupart des images n'atteignent pas le bord du volume de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf5b3e-117e-41d4-94ef-119b60615bcd",
   "metadata": {},
   "source": [
    "### Données attendues\n",
    "\n",
    "Afin d'entraîner les réseaux de neurones dans les meilleurs conditions, les données de la base doivent être normalisées et redimensionnées pour correspondre aux dimensions d'entrée de ces réseaux. Ces données doivent avoir une plage dynamique commune, un contraste proche, et occuper le plus grand volume possible au sein de l'image 3D, sans subir de déformation géométrique.\n",
    "\n",
    "Les données d'un cas sont agrégées en un volume 3D à 4 canaux (un par type d'images IRM), dont la dimension est choisie par lutilisateur, et dans laquelle l'image du cerveau est centrée et est tangente au bord du volume de données dans au moins une des dimensions de l'espace. Les valeurs d'intensité de chaque image sont comprises entre -1 et 1, et la médiane des voxels non nuls vaut 0. Les données dont l'intensité est supérieure à la médiane sont écrasées pour correspondre à ces statistiques.\n",
    "\n",
    "La carte de segmentation est présentée sous la forme d'un volume 3D, dans le même repère que les données, dans lequel chaque voxel est labelisé avec les valeurs suivantes : \n",
    "-  0 : Extérieur du cerveau\n",
    "-  1 : Tissu sain\n",
    "-  2 : *Necrotic and non-enhancing tumor core*\n",
    "-  3 : *Peritumoral edema*\n",
    "-  4 : *GD-enhancing tumor*\n",
    "\n",
    "L'ensemble des informations sur chaque cas (données et segmentation) sont sauvegardées en un unique fichier .npz.'\n",
    "\n",
    "Ci-dessous, l'utilisateur définit la dimension des images qu'il souhaite obtenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110293f1-4a02-473a-b379-d36e0b374b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (240, 240, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af26c1d-1b0b-48cb-aca6-7c6722388d39",
   "metadata": {},
   "source": [
    "### Extraction des données de la base\n",
    "\n",
    "La base de données BraTS 2019 est fournie avec un fichier .csv qui répertorie l'ensemble des cas contenus dans la base. Ce fichier se nomme **'./MICCAI_BraTS_2019_Data_Training/name_mapping.csv'**, et contient entre autre le champ **'Grade'**, qui indique le type de pathologie, et le champ **'BraTS_2019_subject_ID'** qui est l'identifiant du cas. Un second fichier .csv contient les données de survie des patients.\n",
    "\n",
    "Chaque ensemble d'image relatif à un cas est stocké dans le dossier **./MICCAI_BraTS_2019_Data_Training/{Grade}/{BraTS_2019_subject_ID}**, où **{Grade}** et **{BraTS_2019_subject_ID}** sont les valeurs extraites du fichier .csv\n",
    "\n",
    "Dans ces dossiers, les images sont nommées **{BraTS_2019_subject_ID}_{Channel}.nii** où **{Channel}** indique le type d'image (peut être pris parmi la liste ['flair', 't1', 't1ce', 't2', 'seg']).\n",
    "\n",
    "Afin de simplifier la navigation dans la base pour la suite, on construit la liste **path_to_data** qui contient pour chaque cas le chemin vers le dossier contenant les données, ainsi que l'identifiant du cas, utile pour reconstruire le nom des fichiers.\n",
    "\n",
    "Puisque à la fin du pré-traîtement la base de données originale sera supprimée pour réduire le volume de données stockées, on extrait les fichiers .csv du dossier **./MICCAI_BraTS_2019_Data_Training/**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d086dd1d-dfe1-4b22-81cd-f490f105f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déplacement des fichiers .csv\n",
    "!mv ./MICCAI_BraTS_2019_Data_Training/*.csv ./\n",
    "\n",
    "# Récupération des chemins des données\n",
    "name_mapping = pd.read_csv(\"name_mapping.csv\", header=0)\n",
    "path_to_data = []\n",
    "for i in range(name_mapping.shape[0]):\n",
    "    path_to_data.append([os.path.join('MICCAI_BraTS_2019_Data_Training', name_mapping[\"Grade\"][i], name_mapping[\"BraTS_2019_subject_ID\"][i]),name_mapping[\"BraTS_2019_subject_ID\"][i]])                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98459335-8cbb-4e46-8fde-6494a705d4da",
   "metadata": {},
   "source": [
    "## Transformation des images\n",
    "\n",
    "Ici on discute la façon dont les images sont normalisées et déplacées dans une enveloppe commune aux dimensions déterminées précédemment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ec6da-f3d8-4938-8a40-2ef9c878e72e",
   "metadata": {},
   "source": [
    "### Enveloppe parallélépipédique d'une image\n",
    "\n",
    "Les données de la base originale ne remplissent pas tout l'espace qu'elles pourraient, en n'atteignant pas les bords des volumes de données. Après le pré-traîtement, on souhaite que toutes les images aient cette propriété. Pour cela on va rechercher pour chaque cas la plus petite enveloppe parallélépipédique rectangle (suivant les axes de l'image) englobant toutes les images du cas, puis calculer la transformation qui amène cette enveloppe aux bords du volume de données dont on a choisi les dimensions, tout en gardant cette enveloppe centrée, entièrement incluse dans le volume de données, et sans en modifier les proportions.\n",
    "\n",
    "Le calcul de l'enveloppe d'une image est simple : suivant chaque axe, on détermine la plus petite et la plus grande coordonnée pour laquelle l'intensité de l'image est non nulle. Les 6 coordonnées ainsi obtenuent définissent les 6 faces de l'enveloppe parallélépipédique d'une image. Par la suite on calcule la plus petite enveloppe commune à toutes les images d'un cas, en récupérant les valeurs extrèmes obtenues sur chacune des images de ce cas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a6ed6-b9c7-4094-86c8-3df86ff40cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounding_box(img):\n",
    "    # Recherche l'enveloppe parallélépipédique de l'image\n",
    "    \n",
    "    b_box = np.zeros((3,2))\n",
    "    NZ = np.nonzero(img)\n",
    "    for k in range(3):\n",
    "        b_box[k,:] = np.asarray([np.min(NZ[k]), np.max(NZ[k])])\n",
    "\n",
    "    return b_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12716bac-89b3-48d1-9eb8-90e1f9feefdd",
   "metadata": {},
   "source": [
    "### Calcul de la transformation\n",
    "\n",
    "Les fichiers .nii (le format original des images de la base BraTS 2019) contiennent, en plus des informations des voxels et des métadonnées, une matrice nommée *affine* permettant de spécifier la position de l'image dans un système de coordonnées. Les caractéristiques de cette matrice *affine* et ses relations avec le système de coordonnées sont expliquées sur [cette page](https://nipy.org/nibabel/coordinate_systems.html). La librairie NiLearn permet de ré-échantillonner une image dans un autre système de coordonnées. \n",
    "\n",
    "On suppose que la matrice affine de l'image que l'on souhaite obtenir est la matrice identité (matrice carrée de taille 4).\n",
    "\n",
    "Pour déterminer la matrice *affine* de l'image originale dans le repère de l'image souhaitée, on doit calculer la transformation qui amène cette image originale sur l'image souhaitée. On peut décomposer cette transformation en 3 transformations élémentaires : \n",
    "-  Tout d'abord, on translate l'image originale de façon à ce que le centre de l'enveloppe parallélépipédique se retrouve au centre du repère\n",
    "-  On effectue une homotétie par un facteur d'échelle qui permet de grandir ou rétrécir les données pour qu'elles prennent le plus grand volume possible dans le bloc de données final, mais sans que l'enveloppe n'en dépasse.\n",
    "-  On translate de nouveau l'image obtenue pour amener le centre de l'enveloppe transformée au centre du volume de données.\n",
    "\n",
    "Chacune de ces transformations se traduit par une matrice carrée de taille 4, et la transformation globale n'est que le produit matriciel de toutes ces transformations.\n",
    "\n",
    "La fonction ci dessous appelle la fonction de recherche de la plus petite enveloppe commune, puis calcule les 3 matrices des transformations élémentaires, avant de construire la matrice *affine* de la transformation globale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff8e41b-32be-4006-a190-e61e291ce61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_affine_transform(best_b_box, target_shape):\n",
    "    # Calcule la transformation affine 4x4 pour amener les données de leur espace initial vers la taille d'image décidée, sans déformation\n",
    "\n",
    "    scale_vec = target_shape/(best_b_box[:,1]-best_b_box[:,0])\n",
    "    scale_factor = min(scale_vec)\n",
    "    center_target = np.transpose(target_shape/2)\n",
    "    center_img = np.transpose((best_b_box[:,0] + best_b_box[:,1])/2)\n",
    "    \n",
    "    center_affine = np.eye(4)\n",
    "    center_affine[:3,-1] = -center_img\n",
    "    \n",
    "    scale_affine = np.eye(4)*scale_factor\n",
    "    scale_affine[3,3] = 1\n",
    "    \n",
    "    translate_affine = np.eye(4)\n",
    "    translate_affine[:3,-1] = center_target\n",
    "    \n",
    "    target_affine = np.dot(translate_affine, np.dot(scale_affine, center_affine))\n",
    "    \n",
    "    return target_affine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680972-dd93-49c2-8d1a-13f1c7ae5057",
   "metadata": {},
   "source": [
    "### Normalisation des intensités\n",
    "\n",
    "Les images présentes dans la base BraTS 2019 ont toutes des plages d'intensité très différentes les unes des autres. Afin que cela influe le moins possible sur la qualité de l'entraînement des réseaux de neurones, il est nécessaire de normaliser ces intensités.\n",
    "\n",
    "Dans la plupart des images, le tissu tumoral est celui présentant la plus grande intensité, qui parfois est très supérieure à celle des tissus sains. Certains voxels atteignent parfois la valeur de saturation de l'instrument (32767, valeur maximale d'un *int16*). Cependant le volume de tissu tumoral est au moins *presque toujours* inférieur au volume des tissus sains. Puisque tous les voxels situés en dehors du cerveau ont une intensité de 0, l'intensité médiane des voxels non-nuls est une mesure robuste du niveau global de l'image. \n",
    "\n",
    "On souhaite que toutes les intensités soient comprises entre -1 et 1. On va donc réduire les intensités en les divisant par la valeur de la médiane des voxels non nuls, et soustraire 1. Ainsi l'extérieur du cerveau a une intensité de -1 et tous les voxels dont l'intensité est inférieure à la médiane obtiennent une intensité comprise entre -1 et 0. Pour tous les voxels au dessus de cette médiane, on applique un écrasement de la dynamique pour s'assurer que la valeur de l'intensité reste inférieure à 1. La transformation est la suivante : avec $x$ l'intensité  réduite du voxel, on applique : \n",
    "\\begin{equation}\n",
    "x \\rightarrow \n",
    "\\begin{cases}\n",
    "x & \\text{ si } x \\leq 0 &\\\\\n",
    "1-e^{-x} & \\text{ si } x \\gt 0\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Avec cette transformation, on s'assure que l'ordre des intensités est respecté, qu'elles sont comprises entre -1 et 1, et puisque cette transformation est $C^1$, il n'y a pas de rupture de pente dans la courbe des intensités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0c47a8-e7bd-4b4b-a880-b674e0a8ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    img = img/(np.median(img[np.nonzero(img)])) - 1\n",
    "    img = np.where(img > 0, 1-np.exp(-img), img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02230b26-43fd-48da-aff2-c460d0eaf829",
   "metadata": {},
   "source": [
    "## Application du pré-traîtement\n",
    "\n",
    "Dans cette section, on détaille l'ordre dans lequel sont effectuée les opération de pré-traîtement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0ae782-5057-4fab-9abe-0258a60318c3",
   "metadata": {},
   "source": [
    "### Fonction de pré-traîtement d'un cas\n",
    "\n",
    "La fonction ci-dessous s'occupe de réaliser les opérations de pré-traîtement d'un cas.\n",
    "\n",
    "Dans un premier temps, on charge chacune des images relatives à ce cas, et on calcule leurs enveloppes individuelles. Puis on calcule la plus petite enveloppe commune à toutes ces images, avant de s'en servir pour déterminer la matrice *affine* de transformation.\n",
    "\n",
    "Dans un second temps, on identifie les voxels appartenant à du tissus sain. Pour cela, on sélectionne tous les voxels qui sont à la fois non nuls dans au moins une des images IRM, et nuls dans la carte de segmentation.\n",
    "\n",
    "Ensuite, pour chaque image IRM, on normalise les données, et on applique la transformation. Toutes ces images sont regroupées dans un unique tenseur nommé **data**.\n",
    "\n",
    "Enfin, on re-labellise la carte de segmentation pour coller aux spécifications des données attendues, et on lui applique à elle aussi la transformation. Pour éviter que cette carte contienne des valeurs non-entières dues à l'interpolation lors de la transformation, on arrondit chaque voxel à l'entier le plus proche.\n",
    "\n",
    "Enfin, on sauvegarde les données et la carte de segmentation dans un unique fichier nommé **./preprocessed_data/{BraTS_2019_subject_ID}.npz**, où **{BraTS_2019_subject_ID}** est le nom du cas issu du fichier .csv. \n",
    "\n",
    "Cette fonction prend en entrée le chemin du dossier du cas **path**, l'identifiant dans la base qui sert de nom de fichier **file_name** ainsi que la dimension souhaitée des images pré-traitées **target_shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75d12-1a05-461f-9d13-df79d8371230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path, file_name, target_shape):\n",
    "    target_shape = np.asarray(target_shape)\n",
    "    data_tmp = np.zeros((240,240,155,5))\n",
    "    channels = [\"flair\",\"t1\", \"t1ce\", \"t2\", \"seg\"] \n",
    "    bounding_boxes = np.zeros((3,2,5))\n",
    "    \n",
    "    # Chargement des images du cas et calcul des enveloppes individuelles\n",
    "    for k in range(5):\n",
    "        loaded = nib.load(os.path.join(path, file_name + \"_\" + channels[k] + \".nii\"))\n",
    "        img = loaded.get_fdata()\n",
    "        bounding_boxes[:,:,k] = find_bounding_box(img)\n",
    "        data_tmp[:,:,:,k] = img\n",
    "    \n",
    "    # Calcul de la meilleur enveloppe parallélépipédique et de la transformation affine\n",
    "    best_b_box = np.zeros((3,2))\n",
    "    best_b_box[:,0] = bounding_boxes[:,0,:].min(axis=1)\n",
    "    best_b_box[:,1] = bounding_boxes[:,1,:].max(axis=1)\n",
    "    affine = compute_affine_transform(best_b_box, target_shape)\n",
    "    \n",
    "    # Calcul du masque des tissus sains et intégration dans la carte de segmentation\n",
    "    data_tmp[:,:,:,4] = np.where((data_tmp.sum(axis=3) != 0) & (data_tmp[:,:,:,4] == 0), 5, data_tmp[:,:,:,4])\n",
    "    \n",
    "    # Normalisation et recalage des images de données\n",
    "    data = np.zeros((target_shape[0], target_shape[1], target_shape[2], 4))\n",
    "    for k in range(4):\n",
    "        data_tmp[:,:,:,k] = normalize(data_tmp[:,:,:,k])\n",
    "        img = nib.Nifti1Image(data_tmp[:,:,:,k], affine=affine)\n",
    "        data[:,:,:,k] = resample_img(img, target_affine=np.eye(4), target_shape=target_shape, fill_value=-1).get_fdata()\n",
    "    \n",
    "    # Relabellisation et recalage de la nouvelle carte de segmentation\n",
    "    seg_tmp = data_tmp[:,:,:,4]\n",
    "    seg_tmp[seg_tmp == 2] = 3\n",
    "    seg_tmp[seg_tmp == 1] = 2\n",
    "    seg_tmp[seg_tmp == 5] = 1\n",
    "    img = nib.Nifti1Image(seg_tmp, affine=affine)\n",
    "    seg = resample_img(img, target_affine=np.eye(4), target_shape=target_shape, fill_value=0).get_fdata()\n",
    "    seg = np.rint(seg)\n",
    "    \n",
    "    # Sauvegarde des données\n",
    "    np.savez_compressed(os.path.join('preprocessed_data', file_name), data=data.astype(\"float32\"), seg=seg.astype(\"byte\"))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a839b6-45c3-4175-9225-198ffafb262a",
   "metadata": {},
   "source": [
    "### Exécution du pré-traîtement\n",
    "\n",
    "Il ne nous reste plus qu'à boucler sur tous les cas de la base pour leur appliquer le pré-traîtement.\n",
    "\n",
    "Dans un premier temps, s'il n'existe pas encore, le dossier **./preprocessed_data/** est créé.\n",
    "\n",
    "Ensuite, pour chaque paire chemin/identifiant dans la liste **path_to_data**, on applique la fonction de pré-processing. Tous les 5 cas, on affiche les informations d'avancement, de temps écoulé, et de temps restant estimé.\n",
    "\n",
    "Lorsque tous les cas ont été traîtés, on supprime les données de la base originale et on affiche la durée totale du pré-traîtement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234b14c-4165-46d3-bd8d-abafda9335c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('preprocessed_data', exist_ok=True)\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(len(path_to_data)):\n",
    "    path = path_to_data[i][0]\n",
    "    file_name = path_to_data[i][1]\n",
    "    preprocess(path, file_name, target_shape)\n",
    "    \n",
    "    if (i+1)%5 == 0 :\n",
    "        stop_time = time.time()\n",
    "        avancement = (i+1)/(len(path_to_data))\n",
    "        duree = stop_time - start_time\n",
    "        duree_totale = duree / avancement\n",
    "        ETA = duree_totale - duree\n",
    "        print(\"Avancement : {0:.2f}%, Temps écoulé : {1:d}h {2:02d}m {3:02d}s, Temps restant estimé: {4:d}h {5:02d}m {6:02d}s\"\n",
    "              .format(round(100*avancement,2), int(duree)//3600, (int(duree)//60)%60, int(duree)%60, int(ETA)//3600, (int(ETA)//60)%60, int(ETA)%60))\n",
    "\n",
    "# Suppression de la base originale\n",
    "!rm -rf MICCAI_BraTS_2019_Data_Training\n",
    "\n",
    "duree = time.time() - start_time\n",
    "print(\"Pré-traîtement terminé. Durée totale : {0:d}h {1:02d}m {2:02d}s\".format(int(duree)//3600, (int(duree)//60)%60, int(duree)%60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
